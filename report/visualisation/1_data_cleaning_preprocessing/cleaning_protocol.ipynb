{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This file documents the cleaning processes we will employ for our datasets\n",
    "\n",
    "## Adapted from (Data Camp)[https://www.datacamp.com/tutorial/guide-to-data-cleaning-in-python]\n",
    "\n",
    "## 1. Import necessary libraries & modules"
   ],
   "id": "ae834b7b31e6280e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T16:48:33.055067Z",
     "start_time": "2024-11-26T16:48:32.964884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ],
   "id": "17a75c682af09f1b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load the data",
   "id": "76357481011629fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T16:04:37.212716Z",
     "start_time": "2024-11-26T16:04:37.210762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For CSV's:\n",
    "df_for_cleaning = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For Excel files:\n",
    "df_for_cleaning = pd.read_excel('your_dataset.xlsx')\n",
    "\n",
    "# Example for specifying a sheet by name or index\n",
    "df_for_cleaning = pd.read_excel('your_dataset.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Or by index:\n",
    "df_for_cleaning = pd.read_excel('your_dataset.xlsx', sheet_name=0)\n",
    "\n",
    "# For SQL add the following to the list of imports:\n",
    "from sqlalchemy import create_engine\n",
    "# Create an engine for the MySQL database\n",
    "# Replace 'user', 'password', 'host', 'database' with your actual database credentials\n",
    "engine = create_engine('mysql+pymysql://user:password@host/database')\n",
    "\n",
    "# Use pandas to execute a SQL query and load the data into a DataFrame\n",
    "df_for_cleaning = pd.read_sql('SELECT * FROM your_table', con=engine)"
   ],
   "id": "132200198ed7fcc1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Exploring and Understanding the Data",
   "id": "6fbe8fc111df0b66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T16:58:35.775723Z",
     "start_time": "2024-11-26T16:58:35.773058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic info about the dataset\n",
    "df_for_cleaning.info()\n",
    "\n",
    "# Checking the top rows of the dataframe\n",
    "df_for_cleaning.head()\n",
    "\n",
    "# Checking the ending rows of the dataframe\n",
    "df_for_cleaning.tail()\n",
    "\n",
    "# Random Sample of the dataframe\n",
    "df_for_cleaning.sample()\n",
    "\n",
    "# Checking the shape of the dataframe\n",
    "print(df_for_cleaning.shape)\n",
    "\n",
    "# Getting information about number of columns, their names and data types\n",
    "df_for_cleaning.info()\n",
    "\n",
    "# Summary statistics of numerical columns\n",
    "df_for_cleaning.describe()\n",
    "\n",
    "# Check for missing values\n",
    "df_for_cleaning.isnull().sum()\n",
    "\n",
    "# Check for duplicate rows\n",
    "df_for_cleaning.duplicated().sum()\n",
    "\n",
    "# Visualize missing values\n",
    "sns.heatmap(df_for_cleaning.isnull(), cbar=False, cmap='plasma')"
   ],
   "id": "641b208182329a4c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Handling Missing Data\n",
    "### Depending on the data we may need to: \n",
    "#### Remove missing data"
   ],
   "id": "c1fd221637e9357b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove rows with any missing values\n",
    "df_for_cleaning = df_for_cleaning.dropna()\n",
    "\n",
    "# Remove rows with all na values\n",
    "df_for_cleaning.dropna(how='all')\n",
    "\n",
    "# Remove rows where na values meet the specified threshold\n",
    "df_for_cleaning.dropna(thresh=10)\n",
    "\n",
    "# In this instance that we don’t want to include any row that doesn’t have information in a specific column\n",
    "df_for_cleaning.dropna(subset=['colum_name'])\n",
    "\n",
    "# Remove columns with missing values\n",
    "df_for_cleaning = df_for_cleaning.dropna(axis=1, how='all')\n",
    "\n",
    "# Remove rows with a certain threshold of missing values\n",
    "threshold = 0.2\n",
    "df_for_cleaning = df_for_cleaning[df_for_cleaning.isnull().mean(axis=1) < threshold]"
   ],
   "id": "498a9cb877e12a57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Impute missing data i.e. Replace missing values with a calculated value (mean, median, or a specific value).",
   "id": "a83d2d1faa2db8a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:09:01.794702Z",
     "start_time": "2024-11-26T17:09:01.766265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fill missing values with the mean of the column\n",
    "df_for_cleaning['column_name'] = df_for_cleaning['column_name'].fillna(df_for_cleaning['column_name'].mean())\n",
    "\n",
    "# Fill missing values with the median of the column\n",
    "df_for_cleaning['column_name'] = df_for_cleaning['column_name'].fillna(df_for_cleaning['column_name'].median())\n",
    "\n",
    "# Or:\n",
    "col_mean = df_for_cleaning.col_name.mean()\n",
    "df_for_cleaning.duration = df_for_cleaning.col_name.fillna(col_mean)\n",
    "\n",
    "# Fill missing values with a constant value\n",
    "df_for_cleaning['column_name'] = df_for_cleaning['column_name'].fillna(0)\n",
    "\n",
    "# Use forward fill or backward fill\n",
    "df_for_cleaning['column_name'] = df_for_cleaning['column_name'].fillna(method='ffill')\n",
    "df_for_cleaning['column_name'] = df_for_cleaning['column_name'].fillna(method='bfill')"
   ],
   "id": "2ce7b3ca8eea356d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_for_cleaning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fill missing values with the mean of the column\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_for_cleaning[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_for_cleaning\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(df_for_cleaning[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmean())\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Fill missing values with the median of the column\u001B[39;00m\n\u001B[1;32m      5\u001B[0m df_for_cleaning[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_for_cleaning[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(df_for_cleaning[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmedian())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_for_cleaning' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Handling Duplicates",
   "id": "2a4001e250bc4262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check the number of duplicates\n",
    "df_for_cleaning.duplicated().sum()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_for_cleaning = df_for_cleaning.drop_duplicates()\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "df_for_cleaning = df_for_cleaning.drop_duplicates(subset=['column1', 'column2'])"
   ],
   "id": "97e8254edf15a009"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Data Type Conversion\n",
    "### Ensure that columns are of the correct data type (e.g., converting numerical columns that are stored as strings).\n",
    "#### Other formatting inconsistency corrections you may need to conduct include:\n",
    "\n",
    "#### - Unit conversion\n",
    "#### - Email, phone, and address standardization\n",
    "#### - Removing punctuation from strings\n",
    "#### - Using value mapping to address common abbreviations\n"
   ],
   "id": "6fa74725caf0cbc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert a column to numeric\n",
    "df_for_cleaning['column_name'] = pd.to_numeric(df_for_cleaning['column_name'], errors='coerce')\n",
    "\n",
    "# Convert a column to datetime\n",
    "df_for_cleaning['date_column'] = pd.to_datetime(df_for_cleaning['date_column'], errors='coerce')\n",
    "\n",
    "# Convert a column to categorical\n",
    "df_for_cleaning['category_column'] = df_for_cleaning['category_column'].astype('category')"
   ],
   "id": "1cab737b10f2079b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Handling Outliers\n",
    "### Outliers can either be removed or capped.\n",
    "#### Identifying outliers:"
   ],
   "id": "115565096ca6bc0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use box plots or z-scores to detect outliers.\n",
    "\n",
    "# Box plot to visualize outliers\n",
    "sns.boxplot(x=df_for_cleaning['numerical_column'])\n",
    "\n",
    "# Using Z-score to identify outliers\n",
    "df_for_cleaning['z_score'] = zscore(df_for_cleaning['numerical_column'])\n",
    "outliers = df_for_cleaning[df_for_cleaning['z_score'].abs() > 3]"
   ],
   "id": "b611d98151a63dca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Removing Outliers:",
   "id": "e3339e262f9de704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove rows where values are beyond 3 standard deviations from the mean\n",
    "mean = df_for_cleaning['numerical_column'].mean()\n",
    "std_dev = df_for_cleaning['numerical_column'].std()\n",
    "\n",
    "df_cleaned = df_for_cleaning[(df_for_cleaning['numerical_column'] - mean).abs() <= 3 * std_dev]"
   ],
   "id": "29f52b5ec164f7e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Capping outliers to a specific threshold (e.g., 99th percentile)\n",
   "id": "ad94e58031ce3505"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "upper_limit = df_for_cleaning['numerical_column'].quantile(0.99)\n",
    "df_for_cleaning['numerical_column'] = df_for_cleaning['numerical_column'].clip(upper=upper_limit)"
   ],
   "id": "a476b03163b912b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Standardizing or Normalizing Data",
   "id": "a7eb0d2490137ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardizing (z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "df_for_cleaning['scaled_column'] = scaler.fit_transform(df_for_cleaning[['numerical_column']])\n",
    "\n",
    "# Min-max scaling (range 0-1)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_for_cleaning['normalized_column'] = min_max_scaler.fit_transform(df_for_cleaning[['numerical_column']])"
   ],
   "id": "36cd8f6b82e8e27d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Checking for Inconsistencies\n",
   "id": "625d1f0999f85446"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardizing categorical values (e.g., lowercase, no extra spaces)\n",
    "df_for_cleaning['category_column'] = df_for_cleaning['category_column'].str.lower().str.strip()\n",
    "\n",
    "# Replace inconsistent values\n",
    "df_for_cleaning['category_column'] = df_for_cleaning['category_column'].replace({'old_value': 'new_value'})"
   ],
   "id": "4d74b9ae2fa5b2ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Final Review and Summary",
   "id": "1e300698489ef04b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T16:48:43.968254Z",
     "start_time": "2024-11-26T16:48:43.966720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values after cleaning\n",
    "df_for_cleaning.isnull().sum()\n",
    "\n",
    "# Verify the data types again\n",
    "df_for_cleaning.info()\n",
    "\n",
    "# Visualize distributions of cleaned data\n",
    "df_for_cleaning.hist(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "id": "1992ae5d629676e8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12. Saving Cleaned Data",
   "id": "8ff7d085f3520368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "df_for_cleaning.to_csv('2_cleaned_files/cleaned_dataset.csv', index=False)"
   ],
   "id": "240065ed2441aec6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
