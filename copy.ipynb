{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OFCOM Survey Data Analysis & Visualisation\n",
    "## 1. Importing necessary Libraries & Modules\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "plt.close('all')\n",
    "## 2. Loading the OFCOM survey data, Regional COVID Cases, Mortality Data and Vaccination Data and UK cases/mortality data \n",
    "### Loading the OFCOM data:\n",
    "path_for_datasets = '1_data_cleaning_preprocessing/2_cleaned_files/ofcom_survey_data'\n",
    "\n",
    "directory_path = Path(path_for_datasets)\n",
    "\n",
    "file_list = [f.name for f in directory_path.iterdir() if f.is_file()]\n",
    "\n",
    "file_paths = [f'{path_for_datasets}/{file}' for file in file_list if file[-4:] == '.csv']\n",
    "\n",
    "ofcom_dataframes = {}\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "\n",
    "    dataframe['start_date'] = pd.to_datetime(dataframe['start_date'], errors='coerce')\n",
    "    dataframe['end_date'] = pd.to_datetime(dataframe['end_date'], errors='coerce')\n",
    "\n",
    "    dataframe['start_date'] = dataframe['start_date'].dt.date\n",
    "    dataframe['end_date'] = dataframe['end_date'].dt.date\n",
    "\n",
    "    ofcom_dataframes[file_list[i][:-12]] = dataframe\n",
    "\n",
    "dataframe['start_date'] = pd.to_datetime(dataframe['start_date'], errors='coerce')\n",
    "dataframe['end_date'] = pd.to_datetime(dataframe['end_date'], errors='coerce')\n",
    "\n",
    "dataframe['start_date'] = dataframe['start_date'].dt.date\n",
    "dataframe['end_date'] = dataframe['end_date'].dt.date\n",
    "### Loading the ukhsa regional information for mortality, vaccination and cases\n",
    "ukhsa_mortality_dataframe = pd.read_csv(\n",
    "    '1_data_cleaning_preprocessing/2_cleaned_files/ukhsa_mortality_data/uk_regional_covid_mortality_cleaned.csv')\n",
    "ukhsa_cases_dataframe = pd.read_csv(\n",
    "    '1_data_cleaning_preprocessing/2_cleaned_files/ukhsa_cases_data/ukhsa_cases_data_cleaned.csv')\n",
    "ukhsa_vaccination_dataframe = pd.read_csv(\n",
    "    '1_data_cleaning_preprocessing/2_cleaned_files/ukhsa_vaccination_data/ukhsa_vaccination_data_cleaned.csv')\n",
    "### Loading the UK COVID API data \n",
    "covid_api_uk_dataframe = pd.read_csv(\n",
    "    '1_data_cleaning_preprocessing/1_api_connectors_and_csv_parsers/2_processed_databases/covid_19_api_data/17_03_2020_29_11_2024_GBR.csv')\n",
    "covid_api_uk_dataframe = covid_api_uk_dataframe[\n",
    "    covid_api_uk_dataframe['province'].isin(['United Kingdom', 'England', 'Scotland', 'Wales'])]\n",
    "# 3. Frequency of getting information and news on the pandemic\n",
    "## I first get the corresponding dataframe\n",
    "dataframe = ofcom_dataframes['frequency_of_getting_infonews_about_coronavirus_outbreak_in_last_week']\n",
    "## I ensure that the dates are appropriately formatted\n",
    "dataframe['start_date'] = pd.to_datetime(dataframe['start_date'], errors='coerce')\n",
    "dataframe['end_date'] = pd.to_datetime(dataframe['end_date'], errors='coerce')\n",
    "\n",
    "dataframe['start_date'] = dataframe['start_date'].dt.date\n",
    "dataframe['end_date'] = dataframe['end_date'].dt.date\n",
    "## I want to plot the percentage of respondants over time so I need to create a dataframe which contains unique dates and weighted bases from the dataframe and calculate the percentage of respondents\n",
    "weighted_bases = dataframe[dataframe['response'] == 'Weighted base'][['start_date', 'total']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_total(start_date):\n",
    "    lookup_dict = weighted_bases.set_index('start_date')['total'].to_dict()\n",
    "\n",
    "    return lookup_dict.get(start_date)\n",
    "\n",
    "\n",
    "dataframe['weighted_base'] = dataframe['start_date'].map(get_total)\n",
    "dataframe['percentage_total_respondents'] = round((dataframe['total'] / dataframe['weighted_base']) * 100, 2)\n",
    "dataframe = dataframe[['start_date', 'response', 'percentage_total_respondents']]\n",
    "dataframe = dataframe[~dataframe['response'].isin(['Unweighted base', 'NET: At least once a day', 'Weighted base'])]\n",
    "## I then get the comparative data from the COVID-19 API to plot on the graph\n",
    "start_date = min(dataframe['start_date'])\n",
    "start_date\n",
    "end_date = max(dataframe['start_date'])\n",
    "end_date\n",
    "cases_dataframe = covid_api_uk_dataframe.copy()\n",
    "cases_dataframe['date'] = pd.to_datetime(cases_dataframe['date'], errors='coerce')\n",
    "cases_dataframe['date'] = cases_dataframe['date'].dt.date\n",
    "cases_dataframe = cases_dataframe[(cases_dataframe['date'] >= start_date) & (cases_dataframe['date'] <= end_date)]\n",
    "cases_dataframe = cases_dataframe[cases_dataframe['province'] == 'England']\n",
    "cases_dataframe = cases_dataframe[['date', 'confirmed_diff']]\n",
    "lower_percentile = cases_dataframe['confirmed_diff'].quantile(0.01)\n",
    "upper_percentile = cases_dataframe['confirmed_diff'].quantile(0.99)\n",
    "cases_dataframe = cases_dataframe.ffill()\n",
    "df_no_outliers = cases_dataframe[\n",
    "    (cases_dataframe['confirmed_diff'] >= lower_percentile) & (cases_dataframe['confirmed_diff'] <= upper_percentile)]\n",
    "### Finally, I plot the graph:\n",
    "fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "colours = plt.cm.viridis(np.linspace(0, 1, len(dataframe['response'].unique())))\n",
    "\n",
    "for i, category in enumerate(dataframe['response'].unique()):\n",
    "    category_data = dataframe[dataframe['response'] == category]\n",
    "    ax1.plot(category_data['start_date'], category_data['percentage_total_respondents'],\n",
    "             label=category, linestyle='-', color=colours[i])\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Percentage of Total Survey Respondents')\n",
    "\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "ax1.set_xlim([dt.datetime(2020, 3, 27), dt.datetime(2021, 6, 4)])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_no_outliers['date'], df_no_outliers['confirmed_diff'], label='COVID Cases', linestyle='--', color='black')\n",
    "ax2.set_ylabel('Number of COVID Cases Per Day')\n",
    "\n",
    "ax1.legend(title='COVID News Search Frequency:', loc='upper left')\n",
    "ax2.legend(title='Number of reported COVID Cases Per Day', loc='upper right')\n",
    "\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('black')\n",
    "\n",
    "ax1.set_facecolor('white')\n",
    "ax2.set_facecolor('white')\n",
    "\n",
    "lockdown_periods = [\n",
    "    ('2020-03-27', '2020-05-10', '1st Lockdown'),\n",
    "    ('2020-11-05', '2020-12-02', '2nd Lockdown'),\n",
    "    ('2021-01-06', '2021-03-08', '3rd Lockdown'),\n",
    "]\n",
    "\n",
    "for start_date, end_date, label in lockdown_periods:\n",
    "    ax1.axvspan(\n",
    "        dt.datetime.strptime(start_date, '%Y-%m-%d'),\n",
    "        dt.datetime.strptime(end_date, '%Y-%m-%d'),\n",
    "        color='gray', alpha=0.3, label=label\n",
    "    )\n",
    "\n",
    "plt.text(\n",
    "    dt.datetime(2021, 4, 17), 51000,\n",
    "    'Shaded areas\\nrepresent lockdown\\nperiods',\n",
    "    fontsize=10,\n",
    "    color='black',\n",
    "    ha='center',\n",
    "    va='center',\n",
    "    bbox=dict(\n",
    "        facecolor='white',\n",
    "        edgecolor='black',\n",
    "        boxstyle='round,pad=0.5',\n",
    "        alpha=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.title('Time Series of COVID News Search Frequency and Number of COVID Cases')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    '3_final_figures/misinformation_ofcom/frequency_of_sourcing_news/misinformation_frequency_of_news_covid_timeseries.png',\n",
    "    dpi=300)\n",
    "\n",
    "plt.show()\n",
    "# 4. Sources used to get information about the coronavirus outbreak\n",
    "sources_used_dataframe = ofcom_dataframes['sources_used_to_get_infonews_about_coronavirus_outbreak_in_last_week']\n",
    "\n",
    "\n",
    "### As I will be plotting many time series for the graph, I made a function to return the weighted percentages:\n",
    "def calculate_percentage(dataframe):\n",
    "    weighted_bases = dataframe[dataframe['response'] == 'Weighted base'][['start_date', 'total']].reset_index(drop=True)\n",
    "\n",
    "    def get_total(start_date):\n",
    "        lookup_dict = weighted_bases.set_index('start_date')['total'].to_dict()\n",
    "        return lookup_dict.get(start_date)\n",
    "\n",
    "    dataframe['weighted_base'] = dataframe['start_date'].map(get_total)\n",
    "\n",
    "    dataframe['percentage_total_respondents'] = round((dataframe['total'] / dataframe['weighted_base']) * 100, 2)\n",
    "\n",
    "    return dataframe[['start_date', 'response', 'percentage_total_respondents']]\n",
    "\n",
    "\n",
    "sources_used_dataframe = calculate_percentage(sources_used_dataframe)\n",
    "# sources_used_dataframe\n",
    "sources_used_dataframe = sources_used_dataframe[\n",
    "    ~sources_used_dataframe['response'].isin(['Unweighted base', 'NET: At least once a day', 'Weighted base'])]\n",
    "### From the response categories (uncomment) I have selected a few broad categories of interest for different plots\n",
    "# sources_used_dataframe['response'].unique()\n",
    "local_news = [\n",
    "    \"Local sources across TV, radio and online\",\n",
    "    \"Family and friends directly\",\n",
    "    \"Community radio\",\n",
    "    \"People in your local area/neighbourhood\"\n",
    "]\n",
    "newspapers = [\n",
    "    '“Red-top tabloids” such as The Sun or Daily Mirror ( printed )',\n",
    "    '“Red-top tabloids” such as The Sun or Mirror ( online )',\n",
    "    '“Broadsheets” such as The Times or Guardian',\n",
    "    '“Broadsheets” such as The Times or Guardian ( online )',\n",
    "    '“Mid-market tabloids” such as The Daily Mail or Daily Express ( printed )',\n",
    "    '“Mid-market tabloids” such as MailOnline or Express ( online )'\n",
    "]\n",
    "#### After playing with the plotting function, I noted that the response changed from Direct from Government website/ email/ text/ post to Direct from UK Government website/ email/ text/ post\n",
    "official_sources = [\n",
    "    \"Direct from Local health service website/ email/ text/ post\",\n",
    "    \"Direct from NHS website/ email/ text/ post\",\n",
    "    \"Direct from World Health Organisation (WHO) website/ email/ text/ post\",\n",
    "    \"Direct from UK Government website/ email/ text/ post\",\n",
    "    \"Direct from Local council website/ email/ text/ post\",\n",
    "    \"Official scientists\"\n",
    "]\n",
    "sources_used_dataframe.loc[sources_used_dataframe[\n",
    "                               'response'] == \"Direct from Government website/ email/ text/ post\", 'response'] = \"Direct from UK Government website/ email/ text/ post\"\n",
    "television = [\n",
    "    'BBC - TV',\n",
    "    'Channel 4',\n",
    "    'Channel 5',\n",
    "    'ITV'\n",
    "]\n",
    "social_media = [\n",
    "    \"NET: Social Media\",\n",
    "    \"NET: Facebook (Facebook and Facebook Messenger)\",\n",
    "    \"NET: WhatsApp (WhatsApp and WhatsApp groups)\",\n",
    "    \"Instagram\",\n",
    "    \"Twitter\",\n",
    "    \"YouTube\",\n",
    "    \"Snapchat\",\n",
    "]\n",
    "\n",
    "\n",
    "## I created a function to plot similar Time Series graphs to Save time and decide which Pplots are most relevant\n",
    "def plot_time_series(dataframe_1, dataframe_2, categories, filename, plot_title, x_title, y_title, y2_title,\n",
    "                     legend_1_title, legend_2_title, min_date, max_date):\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    colours = plt.cm.viridis_r(np.linspace(0, 1, len(dataframe['response'].unique())))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        category_data = dataframe_1[dataframe_1['response'] == category]\n",
    "        ax1.plot(category_data['start_date'], category_data['percentage_total_respondents'],\n",
    "                 label=category, linestyle='-', color=colours[i])\n",
    "    ax1.grid(False)\n",
    "\n",
    "    for spine in ax1.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "    ax1.set_facecolor('white')\n",
    "\n",
    "    ax1.set_xlabel(f'{x_title}')\n",
    "    ax1.set_ylabel(f'{y_title}')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax2.set_facecolor('white')\n",
    "\n",
    "    ax2.plot(dataframe_2['date'], dataframe_2['confirmed_diff'], label=f'{y2_title}', linestyle='--', color='black')\n",
    "    ax2.set_ylabel('Number of COVID Cases Per Day')\n",
    "\n",
    "    ax1.legend(title=f'{legend_1_title}', loc='upper left')\n",
    "    ax2.legend(title=f'{legend_2_title}', loc='upper right')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    ax1.set_xlim([min_date, max_date])\n",
    "\n",
    "    lockdown_periods = [\n",
    "        ('2020-03-27', '2020-05-10', '1st Lockdown'),\n",
    "        ('2020-11-05', '2020-12-02', '2nd Lockdown'),\n",
    "        ('2021-01-06', '2021-03-08', '3rd Lockdown'),\n",
    "    ]\n",
    "\n",
    "    for start_date, end_date, label in lockdown_periods:\n",
    "        ax1.axvspan(\n",
    "            dt.datetime.strptime(start_date, '%Y-%m-%d'),\n",
    "            dt.datetime.strptime(end_date, '%Y-%m-%d'),\n",
    "            color='gray', alpha=0.3, label=label\n",
    "        )\n",
    "\n",
    "    plt.text(\n",
    "        dt.datetime(2021, 4, 17), 51000,\n",
    "        'Shaded areas\\nrepresent lockdown\\nperiods',\n",
    "        fontsize=10,\n",
    "        color='black',\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        bbox=dict(\n",
    "            facecolor='white',\n",
    "            edgecolor='black',\n",
    "            boxstyle='round,pad=0.5',\n",
    "            alpha=0.3\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.title(f'{plot_title}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'{filename}.png', dpi=300)\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "#### I set the root for the file path for the news_source_figures\n",
    "root_image_path = '3_final_figures/misinformation_ofcom/news_sources_figures/'\n",
    "\n",
    "\n",
    "## I also create two functions to filter the dataframe and return minimum and maximum date\n",
    "def filter_dataframe(dataframe, category):\n",
    "    copy_dataframe = dataframe.copy()\n",
    "    copy_dataframe = copy_dataframe[copy_dataframe['response'].isin(category)]\n",
    "    return copy_dataframe\n",
    "\n",
    "\n",
    "def return_min_max_date(dataframe):\n",
    "    min_date = dataframe['start_date'].min()\n",
    "    max_date = dataframe['start_date'].max()\n",
    "    return min_date, max_date\n",
    "\n",
    "\n",
    "## Plotting the newspaper sources dataframe\n",
    "newspaper_dataframe = filter_dataframe(sources_used_dataframe, newspapers)\n",
    "min_date, max_date = return_min_max_date(newspaper_dataframe)\n",
    "plot_time_series(dataframe_1=newspaper_dataframe, dataframe_2=df_no_outliers, categories=newspapers,\n",
    "                 plot_title='Time Series of Newspaper Source Usage and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents using Newspaper Source',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}newspaper_sources_time_series', min_date=min_date, max_date=max_date)\n",
    "## Plotting the official sources data\n",
    "official_sources_dataframe = filter_dataframe(sources_used_dataframe, official_sources)\n",
    "min_date, max_date = return_min_max_date(official_sources_dataframe)\n",
    "plot_time_series(dataframe_1=official_sources_dataframe, dataframe_2=df_no_outliers, categories=official_sources,\n",
    "                 plot_title='Time Series of Official Source Usage and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents using Official Source',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}official_sources_time_series', min_date=min_date, max_date=max_date)\n",
    "## Plotting the television sources data\n",
    "television_sources_dataframe = filter_dataframe(sources_used_dataframe, television)\n",
    "min_date, max_date = return_min_max_date(television_sources_dataframe)\n",
    "plot_time_series(dataframe_1=television_sources_dataframe, dataframe_2=df_no_outliers, categories=television,\n",
    "                 plot_title='Time Series of Television News Source Usage and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents using Television News Source',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}television_time_series', min_date=min_date, max_date=max_date)\n",
    "## Plotting the social media source usage\n",
    "social_media_dataframe = filter_dataframe(sources_used_dataframe, social_media)\n",
    "min_date, max_date = return_min_max_date(social_media_dataframe)\n",
    "plot_time_series(dataframe_1=social_media_dataframe, dataframe_2=df_no_outliers, categories=social_media,\n",
    "                 plot_title='Time Series of Social Media News Source Usage and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents using Social Media News Source',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}social_media_time_series', min_date=min_date, max_date=max_date)\n",
    "## Plotting the local news source usage\n",
    "\n",
    "local_news_dataframe = filter_dataframe(sources_used_dataframe, local_news)\n",
    "min_date, max_date = return_min_max_date(local_news_dataframe)\n",
    "plot_time_series(dataframe_1=local_news_dataframe, dataframe_2=df_no_outliers, categories=local_news,\n",
    "                 plot_title='Time Series of Local News Source Usage and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents (%)', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents using Local News Source',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}local_news_time_series', min_date=min_date, max_date=max_date)\n",
    "sources_used_dataframe\n",
    "# 5. Most important news source\n",
    "most_important_source_dataframe = ofcom_dataframes['most_important_source_used_in_last_week']\n",
    "most_important_source_dataframe\n",
    "\n",
    "\n",
    "def plot_pie_chart(data, labels, output_file, title, colorscheme='viridis', explode_index=0, figsize=(16, 12)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    colors = plt.colormaps.get_cmap(colorscheme)(np.linspace(0, 0.95, len(labels)))\n",
    "\n",
    "    viridis_colors = plt.cm.viridis(np.linspace(0, 0.95, 16))\n",
    "\n",
    "    explosion = np.zeros(len(data))\n",
    "    explosion[explode_index] = 0.1\n",
    "\n",
    "    _, _, autotexts = plt.pie(\n",
    "        data,\n",
    "        labels=labels,\n",
    "        colors=colors,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140,\n",
    "        labeldistance=1.1,\n",
    "        explode=explosion\n",
    "\n",
    "    )\n",
    "\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "\n",
    "    plt.title(title, fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "## Pie chart of all source frequencies\n",
    "source_frequency = most_important_source_dataframe.copy()\n",
    "source_frequency = source_frequency[['response', 'total']]\n",
    "source_frequency = source_frequency.groupby('response').sum()\n",
    "source_frequency = source_frequency[~source_frequency.index.str.contains('NET:|Weighted base|Unweighted base')]\n",
    "source_frequency = source_frequency.sort_values(by='total', ascending=False)\n",
    "biggest_news_sources = source_frequency[:15]\n",
    "other = source_frequency[15:]\n",
    "other_total = other.sum().item()\n",
    "current_other_row = biggest_news_sources.loc['Other']\n",
    "current_other_row_total = current_other_row.iloc[0].item()\n",
    "current_other_row['total'] = other_total + current_other_row_total\n",
    "biggest_news_sources = biggest_news_sources.sort_values(by='total', ascending=False)\n",
    "data = biggest_news_sources['total']\n",
    "output_filepath = f'3_final_figures/misinformation_ofcom/news_sources_figures/all_sources_piechart.png'\n",
    "title = \"Percieved Most Important News Sources During the Pandemic (All Sources)\"\n",
    "explosion = np.zeros(len(biggest_news_sources['total']))\n",
    "explosion[0] = 0.1\n",
    "plot_pie_chart(\n",
    "    data=data,\n",
    "    labels=biggest_news_sources.index,\n",
    "    output_file=output_filepath,\n",
    "    title=title,\n",
    "    colorscheme='viridis',\n",
    "    explode_index=0,\n",
    "    figsize=(16, 12))\n",
    "# plt.figure(figsize=(16, 12))\n",
    "\n",
    "# viridis_colors = plt.cm.viridis(np.linspace(0, 0.95, 16))\n",
    "\n",
    "# explosion = np.zeros(len(biggest_news_sources['total']))\n",
    "\n",
    "# explosion[0] = 0.1\n",
    "\n",
    "# _, _, autotexts = plt.pie(\n",
    "#     biggest_news_sources['total'], \n",
    "#     labels=biggest_news_sources.index, \n",
    "#     colors=viridis_colors,\n",
    "#     autopct='%1.1f%%',\n",
    "#     startangle=140,\n",
    "#     labeldistance=1.1,\n",
    "#     explode = explosion\n",
    "# )\n",
    "\n",
    "# for autotext in autotexts:\n",
    "#     autotext.set_color('white')\n",
    "\n",
    "# plt.title(\"Percieved Most Important News Sources During the Pandemic (All Sources)\", fontsize=14)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(f'3_final_figures/misinformation_ofcom/news_sources_figures/all_sources_piechart.png', dpi=300)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.close('all')\n",
    "## Pie Chart for grouped source frequencies\n",
    "net_information_sources_most_important = most_important_source_dataframe.copy()\n",
    "net_information_sources_most_important = net_information_sources_most_important[['response', 'total']]\n",
    "net_information_sources_most_important = net_information_sources_most_important.groupby('response').sum()\n",
    "net_information_sources_most_important = net_information_sources_most_important[\n",
    "    net_information_sources_most_important.index.str.contains('NET:')]\n",
    "net_information_sources_most_important = net_information_sources_most_important.sort_values(by='total', ascending=False)\n",
    "net_information_sources_most_important = net_information_sources_most_important[\n",
    "    ~net_information_sources_most_important.index.isin(\n",
    "        ['NET: Offline', 'NET: Closed groups', 'NET: WhatsApp (WhatsApp and WhatsApp groups)'])]\n",
    "data = net_information_sources_most_important['total']\n",
    "output_file = '3_final_figures/misinformation_ofcom/news_sources_figures/net_most_important_piechart.png'\n",
    "title = \"Percieved Most Important News Sources During the Pandemic (Grouped Sources)\"\n",
    "plot_pie_chart(\n",
    "    data=data,\n",
    "    labels=net_information_sources_most_important.index,\n",
    "    output_file=output_file,\n",
    "    title=title,\n",
    "    colorscheme='viridis',\n",
    "    explode_index=0,\n",
    "    figsize=(16, 12))\n",
    "# plt.figure(figsize=(16, 12))\n",
    "\n",
    "# viridis_colors = plt.cm.viridis(np.linspace(0, 1, len(net_information_sources_most_important['total'])))\n",
    "\n",
    "# explosion = np.zeros(len(net_information_sources_most_important['total']))\n",
    "\n",
    "# explosion[0] = 0.1\n",
    "\n",
    "# _, _, autotexts = plt.pie(\n",
    "#     net_information_sources_most_important['total'], \n",
    "#     labels=net_information_sources_most_important.index, \n",
    "#     colors=viridis_colors,\n",
    "#     autopct='%1.1f%%',\n",
    "#     startangle=140,\n",
    "#     labeldistance=1.1,\n",
    "#     explode = explosion\n",
    "# )\n",
    "\n",
    "# for autotext in autotexts:\n",
    "#     autotext.set_color('white')\n",
    "\n",
    "# plt.title(\"Percieved Most Important News Sources During the Pandemic (Grouped Sources)\", fontsize=14)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(f'3_final_figures/misinformation_ofcom/news_sources_figures/net_most_important_piechart.png', dpi=300)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.close('all')\n",
    "## Time series graphs of change in grouped information sources over time\n",
    "most_important_source_dataframe = calculate_percentage(most_important_source_dataframe)\n",
    "most_important_source_dataframe = most_important_source_dataframe[\n",
    "    ~most_important_source_dataframe['response'].isin(['Unweighted base', 'NET: At least once a day', 'Weighted base'])]\n",
    "## Local News information sources\n",
    "local_news_dataframe = filter_dataframe(most_important_source_dataframe, local_news)\n",
    "min_date, max_date = return_min_max_date(local_news_dataframe)\n",
    "plot_time_series(dataframe_1=local_news_dataframe, dataframe_2=df_no_outliers, categories=local_news,\n",
    "                 plot_title='Time Series of Respondants Considering Each Local News Source as The Most Important and COVID Cases',\n",
    "                 x_title='Date', y_title='Percentage of Total Survey Respondents (%)',\n",
    "                 y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents considering Local News Source Most Important',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}most_important_local_news_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## Newspaper information sources\n",
    "newspapers_dataframe = filter_dataframe(most_important_source_dataframe, newspapers)\n",
    "min_date, max_date = return_min_max_date(newspapers_dataframe)\n",
    "plot_time_series(dataframe_1=newspapers_dataframe, dataframe_2=df_no_outliers, categories=newspapers,\n",
    "                 plot_title='Time Series of Respondants Considering Newspaper Source as The Most Important and COVID Cases',\n",
    "                 x_title='Date', y_title='Percentage of Total Survey Respondents (%)',\n",
    "                 y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents considering Newspaper Source Most Important',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}most_important_newspaper_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## Official information sources\n",
    "official_sources_dataframe = filter_dataframe(most_important_source_dataframe, official_sources)\n",
    "min_date, max_date = return_min_max_date(official_sources_dataframe)\n",
    "plot_time_series(dataframe_1=official_sources_dataframe, dataframe_2=df_no_outliers, categories=official_sources,\n",
    "                 plot_title='Time Series of Respondants Considering Official Source as The Most Important and COVID Cases',\n",
    "                 x_title='Date', y_title='Percentage of Total Survey Respondents (%)',\n",
    "                 y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents considering Official Source Most Important',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}most_important_official_source_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## Television Information Sources\n",
    "television_sources_dataframe = filter_dataframe(most_important_source_dataframe, television)\n",
    "min_date, max_date = return_min_max_date(television_sources_dataframe)\n",
    "plot_time_series(dataframe_1=television_sources_dataframe, dataframe_2=df_no_outliers, categories=television,\n",
    "                 plot_title='Time Series of Respondants Considering Television Source as The Most Important and COVID Cases',\n",
    "                 x_title='Date', y_title='Percentage of Total Survey Respondents (%)',\n",
    "                 y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents considering Television Source Most Important',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}most_important_official_source_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## Social Media\n",
    "social_media_sources_dataframe = filter_dataframe(most_important_source_dataframe, social_media)\n",
    "min_date, max_date = return_min_max_date(social_media_sources_dataframe)\n",
    "plot_time_series(dataframe_1=social_media_sources_dataframe, dataframe_2=df_no_outliers, categories=social_media,\n",
    "                 plot_title='Time Series of Respondants Considering Television Source as The Most Important and COVID Cases',\n",
    "                 x_title='Date', y_title='Percentage of Total Survey Respondents (%)',\n",
    "                 y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents considering Social Media Source Most Important',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}most_important_social_media_source_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## 6. Trust in the news\n",
    "trust_in_source_dataframe = ofcom_dataframes[\n",
    "    'trust_in_the_sources_for_informationnews_about_coronavirus_used_in_last_week']\n",
    "trust_in_source_dataframe = calculate_percentage(trust_in_source_dataframe)\n",
    "trust_in_source_dataframe = trust_in_source_dataframe[~trust_in_source_dataframe['response'].isin(\n",
    "    ['Unweighted base', 'Weighted base', 'NET: Trust', 'NET: Do not trust', 'Mean',\n",
    "     'Standard deviation Standard error'])]\n",
    "min_date, max_date = return_min_max_date(trust_in_source_dataframe)\n",
    "categories = trust_in_source_dataframe['response'].unique()\n",
    "plot_time_series(dataframe_1=trust_in_source_dataframe, dataframe_2=df_no_outliers, categories=categories,\n",
    "                 plot_title='Time Series of Trust in News Sources and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents (%)', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents and their Trust Rating',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}trust_in_news_source_time_series', min_date=min_date, max_date=max_date)\n",
    "## 7. Frequency of Exposure to misinformation \n",
    "### Exposure to misinformation over the pandemic\n",
    "exposure_fake_news = ofcom_dataframes[\n",
    "    'whether_came_across_informationnews_about_coronavirus_that_you_think_has_been_false_or_misleading_in_last_week']\n",
    "exposure_fake_news = calculate_percentage(exposure_fake_news)\n",
    "exposure_fake_news = exposure_fake_news[~exposure_fake_news['response'].isin(['Unweighted base', 'Weighted base'])]\n",
    "categories = exposure_fake_news['response'].unique()\n",
    "min_date, max_date = return_min_max_date(exposure_fake_news)\n",
    "plot_time_series(dataframe_1=exposure_fake_news, dataframe_2=df_no_outliers, categories=categories,\n",
    "                 plot_title='Time Series of Exposure to Misinformation and COVID Cases', x_title='Date',\n",
    "                 y_title='Percentage of Total Survey Respondents (%)', y2_title='Number of COVID Cases Per Day',\n",
    "                 legend_1_title='Percentage of Respondents Who Have Been Exposed',\n",
    "                 legend_2_title='Number of reported COVID Cases Per Day',\n",
    "                 filename=f'{root_image_path}exposure_to_misinformation_time_series', min_date=min_date,\n",
    "                 max_date=max_date)\n",
    "## 8. How misinformation has been reported\n",
    "ofcom_dataframes[\n",
    "    'how_theory_that_the_origin_or_cause_of_coronavirus_is_in_some_way_linked_to_5g_technology_has_been_reported']\n",
    "fake_news = [\n",
    "    'how_claims_that_the_coronavirus_vaccine_is_a_cover_for_a_plan_to_implant_trackable_microchips_in_people_have_been_reported',\n",
    "    'how_claims_that_the_coronavirus_vaccine_may_reduce_fertility_have_been_reported',\n",
    "    'how_theory_that_the_origin_or_cause_of_coronavirus_is_in_some_way_linked_to_5g_technology_has_been_reported',\n",
    "    'how_claims_about_injecting_disinfectant_have_been_reported',\n",
    "    'how_claims_about_empty_hospitals_on_social_media_posts_prove_that_coronavirus_has_been_exaggerated_have_been_reported',\n",
    "    'how_claims_about_the_coronavirus_test_which_shows_if_you_currently_have_the_virus_does_not_work_and_93_of_tests_produce_a_false_positive_have_been_reported',\n",
    "    'how_claims_stating_that_the_flu_alone_is_killing_more_people_than_coronavirus_have_been_reported',\n",
    "    'how_claims_about_the_potential_dangers_of_a_coronavirus_vaccine_have_been_reported',\n",
    "    'how_claims_about_face_maskscoverings_offering_no_protection_or_being_harmful_have_been_reported'\n",
    "    ]\n",
    "categories = {'gender': ['male', 'female'], 'age': ['16-24', '18-24', '25-34', '35-44', '45-54', '55-64', '65+'],\n",
    "              'class': ['class_upper_and_middle', 'class_lower_middle', 'class_skilled_working',\n",
    "                        'class_working_class_lowest_grade'],\n",
    "              'region': ['scotland', 'north_east', 'north_west', 'yorkshire_&_humberside', 'west_midlands',\n",
    "                         'east_midlands', 'wales', 'eastern', 'london', 'south_east', 'south_west', 'northern_ireland']}\n",
    "for news in fake_news:\n",
    "    misinformation_dataframe = ofcom_dataframes[news]\n",
    "\n",
    "    for category, category_list in categories.items():\n",
    "\n",
    "        try:\n",
    "\n",
    "            formatted_labels = [c.title().replace('_', ' ') for c in category_list]\n",
    "\n",
    "            copy_df = misinformation_dataframe.copy()\n",
    "\n",
    "            columns = category_list + ['response', 'start_date']\n",
    "            copy_df = copy_df[columns]\n",
    "\n",
    "            aggregated_df = copy_df.groupby(['response'])[category_list].sum().reset_index()\n",
    "\n",
    "            aggregated_df = aggregated_df.loc[~aggregated_df['response'].isin(['Weighted base', 'Unweighted base'])]\n",
    "\n",
    "            groups = tuple(category_list)\n",
    "\n",
    "            response_dict = {}\n",
    "\n",
    "            response_list = []\n",
    "\n",
    "            response_df = aggregated_df['response']\n",
    "\n",
    "            for response in response_df:\n",
    "                response_dict[response] = []\n",
    "                response_list.append(response)\n",
    "\n",
    "            for i, group in enumerate(groups):\n",
    "                response_df = aggregated_df['response']\n",
    "\n",
    "                filter_df = aggregated_df[group]\n",
    "\n",
    "                filter_df = filter_df.loc[~aggregated_df['response'].isin(['Weighted base', 'Unweighted base'])]\n",
    "\n",
    "                for i, value in enumerate(filter_df):\n",
    "                    response_dict[response_list[i]].append(value)\n",
    "\n",
    "            for key, value in response_dict.items():\n",
    "                response_dict[key] = np.array(value)\n",
    "\n",
    "            width = 0.5\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            bottom = np.zeros(len(category_list))\n",
    "\n",
    "            cmap = plt.get_cmap(\"viridis_r\")\n",
    "\n",
    "            positions = np.linspace(0, 1, 3)\n",
    "\n",
    "            colours = [cmap(pos) for pos in positions]\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            for group, response_count in response_dict.items():\n",
    "                p = ax.bar(groups, response_count, width, label=group, bottom=bottom, color=colours[i])\n",
    "                bottom += response_count\n",
    "                i += 1\n",
    "\n",
    "            ax.set_title(f\"{news.replace('_', ' ').title()}\")\n",
    "\n",
    "            ax.legend(loc=\"upper right\")\n",
    "\n",
    "            plt.xticks(ticks=range(len(groups)), labels=formatted_labels, rotation=90)\n",
    "\n",
    "            plt.ylabel(\"Number of Respondents\")\n",
    "\n",
    "            plt.xlabel(category.title())\n",
    "\n",
    "            plt.savefig(f'3_final_figures/misinformation_ofcom/how_false_news_has_been_reported/{news}_{category}.png',\n",
    "                        dpi=300)\n",
    "\n",
    "            plt.close('all')\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"Error when plotting {news} {category} graph\")\n",
    "\n",
    "## 9. Exposure to specific false or misleading recommendations/claims\n",
    "exposure_dataframe = ofcom_dataframes[\n",
    "    'whether_came_across_any_of_these_false_or_misleading_recommendations_about_avoiding_the_coronavirus_in_the_last_week']\n",
    "aggregated_df = exposure_dataframe[\n",
    "    ['response', 'total', 'male', 'female', 'class_upper_and_middle', 'class_lower_middle', 'class_skilled_working',\n",
    "     'class_working_class_lowest_grade']].groupby('response').sum()\n",
    "columns_to_calculate = [\n",
    "    'male',\n",
    "    'female',\n",
    "    'class_upper_and_middle',\n",
    "    'class_lower_middle',\n",
    "    'class_skilled_working',\n",
    "    'class_working_class_lowest_grade'\n",
    "]\n",
    "\n",
    "for column in columns_to_calculate:\n",
    "    aggregated_df[f'{column}_percentage'] = round(\n",
    "        aggregated_df[column] / aggregated_df.loc['Weighted base', column] * 100, 2)\n",
    "aggregated_df = aggregated_df.drop(index=['Unweighted base', 'Weighted base', 'NET: Any'])\n",
    "aggregated_df = aggregated_df.rename(index={\n",
    "    'Increasing use of natural remedies such as colloidal silver, essential oils, garlic, MMS (chlorine dioxide) or vitamin C': 'Increasing use of natural remedies e.g. colloidal silver'})\n",
    "index_order = [i for i in aggregated_df.index if i not in ['Other', 'None of these']] + ['Other', 'None of these']\n",
    "aggregated_df = aggregated_df.reindex(index_order)\n",
    "gender_percentage_misleading_claims = aggregated_df[['male_percentage', 'female_percentage']]\n",
    "plt.figure(figsize=(15, 12))\n",
    "ax = sns.heatmap(gender_percentage_misleading_claims, annot=True, cmap='viridis_r', fmt='.2f', cbar=True,\n",
    "                 cbar_kws={'label': 'Percentage Total Respondents Reporting Exposure'})\n",
    "\n",
    "x_labels = ax.get_xticklabels()\n",
    "new_labels = [label.get_text().replace('_', ' ').title()[:-10] for label in x_labels]\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.title(\"Heatmap of Exposure to Misleading Claims by Gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Misleading Claim\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'3_final_figures/misinformation_ofcom/exposure_to_misinformation/exposure_by_gender.png', dpi=300)\n",
    "\n",
    "plt.close('all')\n",
    "class_percentage_misleading_claims = aggregated_df[\n",
    "    ['class_upper_and_middle_percentage', 'class_lower_middle_percentage', 'class_skilled_working_percentage',\n",
    "     'class_working_class_lowest_grade_percentage']]\n",
    "plt.figure(figsize=(15, 12))\n",
    "ax = sns.heatmap(class_percentage_misleading_claims, annot=True, cmap='viridis_r', fmt='.2f', cbar=True,\n",
    "                 cbar_kws={'label': 'Percentage Total Respondents Reporting Exposure'})\n",
    "\n",
    "x_labels = ax.get_xticklabels()\n",
    "new_labels = [label.get_text().replace('_', ' ').title()[6:-10] for label in x_labels]\n",
    "ax.set_xticklabels(new_labels, rotation=90)\n",
    "\n",
    "plt.title(\"Heatmap of Percentage Exposure of Survey Respondents to Misleading Claims by Social Class\")\n",
    "plt.xlabel(\"Social Class\")\n",
    "plt.ylabel(\"Misleading Claim\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'3_final_figures/misinformation_ofcom/exposure_to_misinformation/exposure_by_class.png', dpi=300)\n",
    "\n",
    "plt.close('all')\n",
    "## 10. Device usage \n",
    "device_used_to_connect = ofcom_dataframes['device_usage_to_connect_to_internet']\n",
    "device_used_to_connect = device_used_to_connect[['response', 'total']]\n",
    "device_used_to_connect = device_used_to_connect.groupby('response').sum()\n",
    "device_used_to_connect = device_used_to_connect.sort_values(by='total', ascending=False)\n",
    "biggest_used_devices = device_used_to_connect[:13]\n",
    "other = device_used_to_connect[13:-1]\n",
    "final_row = device_used_to_connect[-1:]\n",
    "other_total = other.sum().item()\n",
    "current_other_row = biggest_used_devices.loc[\n",
    "    'Other portable/ handheld device (e.g. portable games console/ iPod Touch)']\n",
    "other_total = other.sum().item()\n",
    "\n",
    "current_other_row_total = current_other_row.iloc[0].item()\n",
    "\n",
    "current_other_row['total'] = other_total + current_other_row_total\n",
    "biggest_used_devices = pd.concat([biggest_used_devices, final_row], ignore_index=False)\n",
    "biggest_used_devices = biggest_used_devices.sort_values(by='total', ascending=False)\n",
    "data = biggest_used_devices['total']\n",
    "labels = biggest_used_devices.index\n",
    "biggest_used_devices\n",
    "title = \"Pie Chart Showing the Devices Used by Respondents to Access the Internet.\"\n",
    "output_file = \"3_final_figures/misinformation_ofcom/device_usage/piechart_devices_used_all_respondents\"\n",
    "plot_pie_chart(data=data, labels=labels, output_file=output_file, title=title, colorscheme='viridis', explode_index=0,\n",
    "               figsize=(16, 12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
